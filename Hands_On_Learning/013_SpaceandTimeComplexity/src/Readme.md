# ğŸš€ Time and Space Complexity â€“ A Beginner to Pro-Level Guide

Understanding time and space complexity is critical in designing efficient algorithms and writing scalable software. This document breaks down the concepts in a clean, digestible format.

---

## ğŸ§  What is Time Complexity?

**Time complexity â‰  Time taken to run the program.**

Time complexity is a **mathematical function** that represents **how the execution time of an algorithm grows** with respect to the **input size `N`**.

> âœ… **It is machine-independent.**  
> âœ… **It ignores hardware and environment factors.. **

---

### ğŸ§© Why Time Complexity Matters

- Allows us to compare algorithms independently of hardware.
- Helps in predicting scalability for very large datasets.
- Provides worst-case guarantees.

---

## ğŸ§  What is Space Complexity?

**Space Complexity** refers to the total amount of memory used by an algorithm (including input, auxiliary space, and recursion stack) as a function of the input size `N`.

---

## ğŸ” What to Focus On When Analyzing Complexity?

- âœ… Focus on **worst-case time** complexity.
- âœ… Think about how your algorithm behaves for **large or infinite input sizes**.
- âœ… **Ignore constants** when analyzing growth.  
  E.g., O(2N) â†’ O(N)
- âœ… **Ignore less dominating terms**.  
  E.g., O(NÂ³ + log N) â†’ O(NÂ³)

---

## ğŸ’¡ Example: Linear Search

```
int[] arr = {1, 2, 3, 4, 5};
int target = 4;
for (int i = 0; i < arr.length; i++) {
    if (arr[i] == target) {
        return i;
    }
}
```


## ğŸ§® Big-O Notation O()
### ğŸ”¤ Word-Level Definition:
* O(NÂ³) means the algorithmâ€™s runtime will not exceed NÂ³ operations for large inputs.

* It can still run faster (e.g., O(NÂ²), O(N log N)) for certain inputs â€” but it never grows beyond NÂ³.

* Big-O is an upper bound, it tells us the worst-case scenario.


### Mathematical Definition: 
* lim
x->infinity f(x)/g(x) < 0
 
### ğŸ§ª Common Complexities Table

| Complexity | Name         | Example Scenarios                        |
| ---------- | ------------ | ---------------------------------------- |
| O(1)       | Constant     | Accessing array by index                 |
| O(log N)   | Logarithmic  | Binary Search                            |
| O(N)       | Linear       | Linear Search, Sum of Array              |
| O(N log N) | Linearithmic | Merge Sort, Quick Sort (avg case)        |
| O(NÂ²)      | Quadratic    | Bubble Sort, Insertion Sort              |
| O(2^N)     | Exponential  | Recursive Fibonacci, Subset Problems     |
| O(N!)      | Factorial    | Traveling Salesman, N-Queens Brute Force |


## ğŸ§­ Big-Omega Notation `Î©()`

### ğŸ”¤ Word-Level Definition:

- **Big-Omega (Î©)** represents the **best-case lower bound** of an algorithm's time or space complexity.
- It tells us the **minimum number of operations** that an algorithm will perform **regardless of optimizations or lucky inputs**.
- Just like Big-O is used for worst-case, **Big-Omega is for best-case**.

> ğŸ’¬ Example:  
> In **Linear Search**,
> - Best Case: The element is at the first index â†’ 1 operation â†’ **Î©(1)**
> - Worst Case: Element not present or at end â†’ **O(N)**

---

### ğŸ§  Mathematical Definition of Big-Omega:

Let `f(N)` be the actual time taken by the algorithm.

We say:

f(N) = Î©(g(N)) if there exist constants C > 0 and Nâ‚€ such that:
f(N) â‰¥ C * g(N) for all N â‰¥ Nâ‚€


Where:
- `g(N)` is the bounding function from below.
- `C` is a positive constant.
- `Nâ‚€` is the input size after which this lower bound holds.

---

### ğŸ§ª When to Use Big-Omega?

- To describe the **best performance guarantee**.
- Important in **theoretical analysis** and **algorithm lower bounds**.
- Useful when showing **no algorithm can do better than Î©(N log N)** for certain problems (like comparison-based sorting).

---

### ğŸ§  Common Examples

| Algorithm           | Best Case       | Î© Notation     |
|---------------------|------------------|-----------------|
| Linear Search       | Element at start | Î©(1)            |
| Binary Search       | First guess hit  | Î©(1)            |
| Insertion Sort      | Sorted input     | Î©(N)            |
| Merge Sort          | Always same      | Î©(N log N)      |
| Recursive Fibonacci | Recursive Tree   | Î©(2^N)          |

---

ğŸ“Œ **Note**:  
Big-Omega gives you **hope** â€” but in production, we must code defensively and optimize for **Big-O (worst-case)**, not just Omega.

---


## âš–ï¸ Big-Theta Notation `Î˜()`

### ğŸ”¤ Word-Level Definition:

- **Big-Theta (Î˜)** describes the **tight bound** on an algorithmâ€™s growth.
- It gives both:
  - **Lower bound** (like Î©)
  - **Upper bound** (like O)

> In simple terms:  
> The algorithm **always** takes this amount of time â€” **not more, not less** â€” up to a constant factor.

So, if an algorithm is Î˜(NÂ²), that means:
- It **wonâ€™t take less** than NÂ² time (Î©)
- It **wonâ€™t take more** than NÂ² time (O)
- Hence, it **always behaves like NÂ²** for large input sizes

---

### ğŸ§  Mathematical Definition of Big-Theta:

Let `f(N)` be the time or space function of the algorithm.

We say:
```

f(N) = Î˜(g(N)) if there exist constants Câ‚ > 0, Câ‚‚ > 0, and Nâ‚€ such that:
Câ‚ \* g(N) â‰¤ f(N) â‰¤ Câ‚‚ \* g(N)   for all N â‰¥ Nâ‚€

```

This tells us:
- The function is **sandwiched** between two constant multiples of `g(N)`
- Itâ€™s a **tight asymptotic bound**

---

### ğŸ“Œ Why Big-Theta is Important

- It **precisely characterizes** an algorithm's behavior.
- Useful when **best-case and worst-case** are the same (e.g., Merge Sort).
- Helps in understanding **algorithm efficiency in all scenarios**, not just optimistic or pessimistic cases.

---

### ğŸ§ª Common Examples

| Algorithm           | Tight Bound         | Î˜ Notation      |
|---------------------|----------------------|------------------|
| Merge Sort          | Always Î˜(N log N)    | Î˜(N log N)       |
| Binary Search       | Always Î˜(log N)      | Î˜(log N)         |
| Linear Search       | Worst O(N), Best Î©(1)| âŒ Not Î˜(N)       |
| Insertion Sort      | Best Î©(N), Worst O(NÂ²) | âŒ Not Î˜(NÂ²)     |
| Selection Sort      | Always Î˜(NÂ²)         | Î˜(NÂ²)            |

---

### âœ… When Can We Use Î˜?

- Only when **best-case = worst-case = average-case (asymptotically)**
- Cannot be used if algorithm has **varying performance across inputs**

---

ğŸ“š **Rule of Thumb**  
> Use Î˜ when the algorithmâ€™s time or space complexity is **well-characterized and stable** for all input patterns.

---






